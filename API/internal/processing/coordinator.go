package processing

import (
	"context"
	"fmt"
	"log"
	"sync/atomic"
	"time"

	"motion-index-fiber/internal/config"
	"motion-index-fiber/internal/hardware"
	"motion-index-fiber/pkg/cloud/digitalocean"
	"motion-index-fiber/pkg/processing/classifier"
	"motion-index-fiber/pkg/processing/extractor"
	"motion-index-fiber/pkg/processing/gpu"
	"motion-index-fiber/pkg/processing/queue"
	"motion-index-fiber/pkg/storage"
)

// DocumentCoordinator orchestrates document processing workflows
// Following UNIX philosophy: coordinate small, focused tools
type DocumentCoordinator struct {
	config       *config.Config
	workerConfig *hardware.WorkerConfig
	hardwareInfo *hardware.Analysis
	
	// Services
	storage    storage.Service
	extractor  extractor.Service
	classifier classifier.Service
	search     interface{} // Search service interface
	gpu        gpu.GPUAccelerator
	
	// Queue management
	queueManager queue.QueueManager
	
	// Metrics
	processed *int64
	errors    *int64
	skipped   *int64
}

// CoordinatorConfig holds configuration for document coordination
type CoordinatorConfig struct {
	Config       *config.Config
	WorkerConfig *hardware.WorkerConfig
	HardwareInfo *hardware.Analysis
}

// NewDocumentCoordinator creates a new document processing coordinator
func NewDocumentCoordinator(cfg *CoordinatorConfig) (*DocumentCoordinator, error) {
	coordinator := &DocumentCoordinator{
		config:       cfg.Config,
		workerConfig: cfg.WorkerConfig,
		hardwareInfo: cfg.HardwareInfo,
		processed:    new(int64),
		errors:       new(int64),
		skipped:      new(int64),
	}
	
	if err := coordinator.initializeServices(); err != nil {
		return nil, fmt.Errorf("failed to initialize services: %w", err)
	}
	
	if err := coordinator.initializeQueues(); err != nil {
		return nil, fmt.Errorf("failed to initialize queues: %w", err)
	}
	
	return coordinator, nil
}

// initializeServices initializes all required services
func (c *DocumentCoordinator) initializeServices() error {
	// Initialize DigitalOcean provider
	provider, err := digitalocean.NewProviderFromEnvironment()
	if err != nil {
		return fmt.Errorf("failed to create DigitalOcean provider: %w", err)
	}
	
	if err := provider.Initialize(); err != nil {
		return fmt.Errorf("failed to initialize provider: %w", err)
	}
	
	services := provider.GetServices()
	c.storage = services.Storage
	c.search = services.Search
	
	// Initialize extraction service
	c.extractor = extractor.NewService()
	
	// Initialize classification service
	classifierInstance, err := classifier.NewService(&classifier.Config{
		Provider: "ollama",
		APIKey:   "", // Not needed for Ollama
		Model:    c.config.AI.Ollama.Model,
	})
	if err != nil {
		return fmt.Errorf("failed to initialize classifier: %w", err)
	}
	c.classifier = classifierInstance
	
	// Initialize GPU accelerator
	gpuInstance, err := gpu.NewNVIDIAAccelerator(&gpu.GPUConfig{
		Enabled:       true,
		FallbackToCPU: true,
		BatchSize:     32,
	})
	if err != nil {
		return fmt.Errorf("failed to initialize GPU accelerator: %w", err)
	}
	c.gpu = gpuInstance
	
	return nil
}

// initializeQueues sets up processing queues with optimal configuration
func (c *DocumentCoordinator) initializeQueues() error {
	c.queueManager = queue.NewQueueManager()
	
	ctx := context.Background()
	
	// Extraction queue - CPU intensive
	_, err := c.queueManager.CreateQueue(&queue.QueueConfig{
		Name:           "extraction",
		Type:           queue.QueueTypeExtraction,
		MaxSize:        1000,
		WorkerCount:    c.workerConfig.Extraction,
		ProcessTimeout: 5 * time.Minute,
		RetryAttempts:  3,
		RetryDelay:     10 * time.Second,
		EnableMetrics:  true,
	}, c.createExtractionProcessor())
	
	if err != nil {
		return fmt.Errorf("failed to create extraction queue: %w", err)
	}
	
	// Classification queue - API rate limited
	_, err = c.queueManager.CreateQueue(&queue.QueueConfig{
		Name:            "classification",
		Type:            queue.QueueTypeClassification,
		MaxSize:         500,
		WorkerCount:     c.workerConfig.Classification,
		ProcessTimeout:  2 * time.Minute,
		RetryAttempts:   5,
		RetryDelay:      30 * time.Second,
		EnableRateLimit: true,
		RateLimit:       50, // 50 requests per minute
		BurstSize:       10,
		EnableMetrics:   true,
	}, c.createClassificationProcessor())
	
	if err != nil {
		return fmt.Errorf("failed to create classification queue: %w", err)
	}
	
	// Indexing queue - Network I/O
	_, err = c.queueManager.CreateQueue(&queue.QueueConfig{
		Name:           "indexing",
		Type:           queue.QueueTypeIndexing,
		MaxSize:        1000,
		WorkerCount:    c.workerConfig.Indexing,
		ProcessTimeout: 1 * time.Minute,
		RetryAttempts:  3,
		RetryDelay:     5 * time.Second,
		EnableMetrics:  true,
	}, c.createIndexingProcessor())
	
	if err != nil {
		return fmt.Errorf("failed to create indexing queue: %w", err)
	}
	
	// Start queue manager
	return c.queueManager.Start(ctx)
}

// ClassifyAll processes all documents in storage
func (c *DocumentCoordinator) ClassifyAll(ctx context.Context) error {
	log.Println("ðŸ“‹ Listing all documents from storage...")
	
	objects, err := c.storage.List(ctx, "")
	if err != nil {
		return fmt.Errorf("failed to list objects: %w", err)
	}
	
	log.Printf("ðŸš€ Processing %d documents for classification...", len(objects))
	
	return c.processDocuments(ctx, objects)
}

// ClassifyBatch processes documents in specified batch size
func (c *DocumentCoordinator) ClassifyBatch(ctx context.Context, batchSize int) error {
	log.Printf("ðŸ“‹ Listing documents for batch processing (limit: %d)...", batchSize)
	
	objects, err := c.storage.List(ctx, "")
	if err != nil {
		return fmt.Errorf("failed to list objects: %w", err)
	}
	
	// Limit to batch size
	if len(objects) > batchSize {
		objects = objects[:batchSize]
	}
	
	log.Printf("ðŸš€ Processing %d documents in batch...", len(objects))
	
	return c.processDocuments(ctx, objects)
}

// ClassifyFiles processes specific files
func (c *DocumentCoordinator) ClassifyFiles(ctx context.Context, files []string) error {
	log.Printf("ðŸš€ Processing %d specific files...", len(files))
	
	// Convert file paths to storage objects
	objects := make([]*storage.StorageObject, len(files))
	for i, file := range files {
		objects[i] = &storage.StorageObject{
			Path: file,
		}
	}
	
	return c.processDocuments(ctx, objects)
}

// processDocuments handles the core document processing workflow
func (c *DocumentCoordinator) processDocuments(ctx context.Context, objects []*storage.StorageObject) error {
	totalDocuments := len(objects)
	if totalDocuments == 0 {
		log.Println("âœ… No documents to process")
		return nil
	}
	
	// Start progress monitoring
	stopMonitor := make(chan bool)
	go c.monitorProgress(stopMonitor, int64(totalDocuments))
	defer func() { stopMonitor <- true }()
	
	// Process documents through the pipeline
	for _, obj := range objects {
		select {
		case <-ctx.Done():
			return ctx.Err()
		default:
			// Queue document for processing
			if err := c.queueDocument(ctx, obj); err != nil {
				log.Printf("âš ï¸ Failed to queue document %s: %v", obj.Path, err)
				atomic.AddInt64(c.errors, 1)
				continue
			}
		}
	}
	
	// Wait for all processing to complete
	if err := c.waitForCompletion(ctx); err != nil {
		return fmt.Errorf("processing incomplete: %w", err)
	}
	
	c.printSummary()
	return nil
}

// queueDocument adds a document to the processing pipeline
func (c *DocumentCoordinator) queueDocument(ctx context.Context, obj *storage.StorageObject) error {
	// Create processing job
	job := &ProcessingJob{
		DocumentKey: obj.Path,
		DocumentPath: obj.Path,
		Timestamp:   time.Now(),
	}
	
	// Convert to queue item
	queueItem := &queue.QueueItem{
		ID:        fmt.Sprintf("doc-%d", time.Now().UnixNano()),
		Type:      queue.QueueTypeExtraction,
		Priority:  queue.PriorityNormal,
		Data:      job,
		Metadata:  map[string]interface{}{"document_path": obj.Path},
		CreatedAt: time.Now(),
		MaxRetries: 3,
	}
	
	// Start with extraction
	extractionQueue, err := c.queueManager.GetQueue("extraction")
	if err != nil {
		return err
	}
	
	return extractionQueue.Enqueue(ctx, queueItem)
}

// waitForCompletion waits for all queues to finish processing
func (c *DocumentCoordinator) waitForCompletion(ctx context.Context) error {
	ticker := time.NewTicker(5 * time.Second)
	defer ticker.Stop()
	
	for {
		select {
		case <-ctx.Done():
			return ctx.Err()
		case <-ticker.C:
			allEmpty := true
			for _, queueName := range c.queueManager.ListQueues() {
				queue, err := c.queueManager.GetQueue(queueName)
				if err != nil {
					continue
				}
				if !queue.IsEmpty() {
					allEmpty = false
					break
				}
			}
			if allEmpty {
				return nil
			}
		}
	}
}

// monitorProgress provides real-time progress updates
func (c *DocumentCoordinator) monitorProgress(stop <-chan bool, total int64) {
	ticker := time.NewTicker(10 * time.Second)
	defer ticker.Stop()
	
	for {
		select {
		case <-stop:
			return
		case <-ticker.C:
			processed := atomic.LoadInt64(c.processed)
			errors := atomic.LoadInt64(c.errors)
			skipped := atomic.LoadInt64(c.skipped)
			
			progress := float64(processed+errors+skipped) / float64(total) * 100
			
			log.Printf("ðŸ“Š Progress: %.1f%% (%d/%d) | âœ… %d processed | âŒ %d errors | â­ï¸ %d skipped",
				progress, processed+errors+skipped, total, processed, errors, skipped)
		}
	}
}

// printSummary displays final processing statistics
func (c *DocumentCoordinator) printSummary() {
	processed := atomic.LoadInt64(c.processed)
	errors := atomic.LoadInt64(c.errors)
	skipped := atomic.LoadInt64(c.skipped)
	
	fmt.Println("\nðŸ“Š Processing Summary")
	fmt.Println("====================")
	fmt.Printf("âœ… Processed: %d\n", processed)
	fmt.Printf("âŒ Errors: %d\n", errors)
	fmt.Printf("â­ï¸ Skipped: %d\n", skipped)
	fmt.Printf("ðŸ“ˆ Total: %d\n", processed+errors+skipped)
}

// createExtractionProcessor creates the extraction processor function
func (c *DocumentCoordinator) createExtractionProcessor() queue.ProcessorFunc {
	return func(ctx context.Context, item *queue.QueueItem) *queue.ProcessingResult {
		log.Printf("ðŸ” Processing extraction for item: %s", item.ID)
		
		// TODO: Implement actual extraction logic
		result := &queue.ProcessingResult{
			Success:  true,
			Duration: time.Since(item.CreatedAt),
			Output:   "extracted text placeholder",
		}
		
		atomic.AddInt64(c.processed, 1)
		return result
	}
}

// createClassificationProcessor creates the classification processor function
func (c *DocumentCoordinator) createClassificationProcessor() queue.ProcessorFunc {
	return func(ctx context.Context, item *queue.QueueItem) *queue.ProcessingResult {
		log.Printf("ðŸ·ï¸ Processing classification for item: %s", item.ID)
		
		// TODO: Implement actual classification logic
		result := &queue.ProcessingResult{
			Success:  true,
			Duration: time.Since(item.CreatedAt),
			Output:   "classification placeholder",
		}
		
		atomic.AddInt64(c.processed, 1)
		return result
	}
}

// createIndexingProcessor creates the indexing processor function
func (c *DocumentCoordinator) createIndexingProcessor() queue.ProcessorFunc {
	return func(ctx context.Context, item *queue.QueueItem) *queue.ProcessingResult {
		log.Printf("ðŸ“š Processing indexing for item: %s", item.ID)
		
		// TODO: Implement actual indexing logic
		result := &queue.ProcessingResult{
			Success:  true,
			Duration: time.Since(item.CreatedAt),
			Output:   "indexed successfully",
		}
		
		atomic.AddInt64(c.processed, 1)
		return result
	}
}

// Close shuts down the coordinator and cleans up resources
func (c *DocumentCoordinator) Close() error {
	if c.queueManager != nil {
		return c.queueManager.Stop(context.Background())
	}
	return nil
}

// ProcessingJob represents a document processing job
type ProcessingJob struct {
	DocumentKey  string    `json:"document_key"`
	DocumentPath string    `json:"document_path"`
	Timestamp    time.Time `json:"timestamp"`
	
	// Processing results
	ExtractedText string                 `json:"extracted_text,omitempty"`
	Classification string               `json:"classification,omitempty"`
	Metadata      map[string]interface{} `json:"metadata,omitempty"`
}